{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Start the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gym_unity.envs import UnityEnv\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Before running the code cell below_**, change the `file_name` parameter to match the location of the Reacher Unity environment.\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_name = 'unity_envs/Crawler_StaticTarget_Linux/Crawler_StaticTarget_Linux.x86_64'\n",
    "#env_name = 'unity_envs/Crawler_StaticTarget'\n",
    "#env = UnityEnv(env_name,worker_id=1,use_visual=False, multiagent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv, VecEnv\n",
    "\n",
    "\n",
    "# def make_env():\n",
    "#     def _thunk():\n",
    "#         env = gym.make('Pendulum-v0')\n",
    "#         return env\n",
    "#     return _thunk\n",
    "\n",
    "# num_envs = 4\n",
    "# envs = [make_env() for _ in range(num_envs)]\n",
    "# env = SubprocVecEnv(envs) \n",
    "\n",
    "# #env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "* Set-up: A creature with 4 arms and 4 forearms.\n",
    "* Goal: The agents must move its body toward the goal direction without falling.\n",
    "* CrawlerStaticTarget - Goal direction is always forward.\n",
    "* CrawlerDynamicTarget- Goal direction is randomized.\n",
    "* Agents: The environment contains 3 agent linked to a single Brain.\n",
    "* Agent Reward Function (independent):\n",
    "* +0.03 times body velocity in the goal direction.\n",
    "* +0.01 times body direction alignment with goal direction.\n",
    "* Brains: One Brain with the following observation/action space.\n",
    "* Vector Observation space: 117 variables corresponding to position, rotation, velocity, and angular velocities of each limb plus the acceleration and angular acceleration of the body.\n",
    "* Vector Action space: (Continuous) Size of 20, corresponding to target rotations for joints.\n",
    "* Visual Observations: None.\n",
    "* Reset Parameters: None\n",
    "* Benchmark Mean Reward for CrawlerStaticTarget: 2000\n",
    "* Benchmark Mean Reward for CrawlerDynamicTarget: 400\n",
    "\n",
    "Lets print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 1\n",
      "There are 1 agents. Each observes a state with length: 3\n",
      "The state for the first agent looks like: [ 0.93577375 -0.35260105  0.3393941 ]\n"
     ]
    }
   ],
   "source": [
    "# number of agents\n",
    "#num_agents = env.number_agents\n",
    "num_agents = 1\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = env.action_space.shape[0]\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env.reset()\n",
    "state_size = env.observation_space.shape[0]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(num_agents, state_size))\n",
    "print('The state for the first agent looks like:', states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: -1066.4518175517956\n"
     ]
    }
   ],
   "source": [
    "states = env.reset()                 # reset env and get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)        # initialize the score (for each agent)\n",
    "step=0\n",
    "while True:\n",
    "    # select an action (for each agent)\n",
    "    actions = list(2*np.random.rand(num_agents, action_size)-1)\n",
    "    next_states,rewards,dones,_ = env.step(actions)    \n",
    "    \n",
    "    # update the score (for each agent)\n",
    "    scores +=  rewards\n",
    "    \n",
    "    # roll over states to next time step\n",
    "    states = next_states                               \n",
    "    step+=1\n",
    "    \n",
    "    #print (rewards,dones)\n",
    "    # exit loop if episode finished\n",
    "    if np.any(dones):\n",
    "        break\n",
    "\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training the agent!\n",
    "\n",
    "Now it's turn to train an agent to solve the environment!  When training the environment, we have to set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import datetime\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# imports for rendering outputs in Jupyter.\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0\n"
     ]
    }
   ],
   "source": [
    "# defining the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (\"using\",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define policy network (Actor Critic style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_low = env.action_space.low\n",
    "action_high = env.action_space.high\n",
    "\n",
    "# define actor critic network\n",
    "class ActorCritic(nn.Module):\n",
    "    \n",
    "    def __init__(self,state_size,action_size,action_high,action_low,hidden_size=32):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # action range\n",
    "        self.action_high = torch.tensor(action_high).to(device)\n",
    "        self.action_low = torch.tensor(action_low).to(device)\n",
    "        \n",
    "        self.std = nn.Parameter(torch.zeros(action_size))\n",
    "        \n",
    "        # common network\n",
    "        self.fc1 = nn.Linear(state_size,512)\n",
    "        \n",
    "        # actor network\n",
    "        self.fc2_actor = nn.Linear(512,256)\n",
    "        self.fc3_action = nn.Linear(256,action_size)\n",
    "        #self.fc3_std = nn.Linear(64,action_size)\n",
    "        \n",
    "        # critic network\n",
    "        self.fc2_critic = nn.Linear(512,256)\n",
    "        self.fc3_critic = nn.Linear(256,1)\n",
    "    \n",
    "    def forward(self,state):\n",
    "        # common network\n",
    "        x = F.relu(self.fc1(state))\n",
    "        \n",
    "        # actor network\n",
    "        x_actor = F.relu(self.fc2_actor(x))\n",
    "        action_mean = F.sigmoid(self.fc3_action(x_actor))\n",
    "        ## rescale action mean\n",
    "        action_mean_ = (self.action_high-self.action_low)*action_mean + self.action_low\n",
    "        #action_std = F.sigmoid(self.fc3_std(x_actor))\n",
    "        \n",
    "        # critic network\n",
    "        x_critic = F.relu(self.fc2_critic(x))\n",
    "        v = self.fc3_critic(x_critic)\n",
    "        return action_mean_,v\n",
    "    \n",
    "    def act(self,state,action=None):\n",
    "        # converting state from numpy array to pytorch tensor on the \"device\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        action_mean,v = self.forward(state)\n",
    "        prob_dist = Normal(action_mean,F.softplus(self.std))\n",
    "        if action is None:\n",
    "            action = prob_dist.sample()\n",
    "        log_prob = prob_dist.log_prob(action).sum(-1).unsqueeze(-1)\n",
    "        entropy = prob_dist.entropy().sum(-1).unsqueeze(-1)\n",
    "        return {'a': action,\n",
    "                'log_pi_a': log_prob,\n",
    "                'ent': entropy,\n",
    "                'mean': action_mean,\n",
    "                'v': v}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Storage class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    def __init__(self, size, keys=None):\n",
    "        if keys is None:\n",
    "            keys = []\n",
    "        keys = keys + ['s', 'a', 'r', 'm',\n",
    "                       'v', 'q', 'pi', 'log_pi', 'ent',\n",
    "                       'adv', 'ret', 'q_a', 'log_pi_a',\n",
    "                       'mean']\n",
    "        self.keys = keys\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def add(self, data):\n",
    "        for k, v in data.items():\n",
    "            assert k in self.keys\n",
    "            getattr(self, k).append(v)\n",
    "\n",
    "    def placeholder(self):\n",
    "        for k in self.keys:\n",
    "            v = getattr(self, k)\n",
    "            if len(v) == 0:\n",
    "                setattr(self, k, [None] * self.size)\n",
    "\n",
    "    def reset(self):\n",
    "        for key in self.keys:\n",
    "            setattr(self, key, [])\n",
    "\n",
    "    def cat(self, keys):\n",
    "        data = [getattr(self, k)[:self.size] for k in keys]\n",
    "        return map(lambda x: torch.cat(x, dim=0), data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from itertools import accumulate\n",
    "import torch.tensor as tensor\n",
    "\n",
    "def random_sample(indices, batch_size):\n",
    "    indices = np.asarray(np.random.permutation(indices))\n",
    "    batches = indices[:len(indices) // batch_size * batch_size].reshape(-1, batch_size)\n",
    "    for batch in batches:\n",
    "        yield batch\n",
    "    r = len(indices) % batch_size\n",
    "    if r:\n",
    "        yield indices[-r:]\n",
    "        \n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self,env,learning_rate=1e-3):\n",
    "        self.env = env\n",
    "        nS = state_size\n",
    "        nA = action_size\n",
    "        self.policy = ActorCritic(state_size=nS,hidden_size=128,action_size=nA,\n",
    "                             action_low=action_low,action_high=action_high).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate, eps=1e-5)\n",
    "        \n",
    "        # reset the environment\n",
    "        self.states = np.array(self.env.reset()).reshape(1,-1)\n",
    "        \n",
    "        self.episode_rewards_window = deque(maxlen=100)\n",
    "        self.episode_rewards = []\n",
    "        num_trajectories = 1\n",
    "        self.online_rewards = np.zeros(num_trajectories)\n",
    "    \n",
    "        \n",
    "    def train(self,max_opt_steps=1000,num_trajectories=1,rollout_length=2048,mini_batch_size=64,gamma=.99,\n",
    "              target_score=-250,use_gae=False,gae_tau=0.95,PRINT_EVERY=100):\n",
    "        \n",
    "        for opt_step in range(max_opt_steps):\n",
    "        \n",
    "            storage = Storage(rollout_length)\n",
    "            for _ in range(rollout_length):\n",
    "                \n",
    "                prediction = self.policy.act(self.states)\n",
    "                \n",
    "                # send all actions to tne environment\n",
    "                next_states,rewards,terminals,_ = self.env.step(prediction['a'].cpu().numpy()[0])\n",
    "                \n",
    "                \n",
    "                next_states = np.array(next_states).reshape(1,-1)\n",
    "                rewards = np.array(rewards).reshape(-1)               \n",
    "                terminals = np.array(terminals).reshape(-1)\n",
    "                \n",
    "                #print (self.online_rewardsn.shape,)\n",
    "                self.online_rewards += rewards\n",
    "                \n",
    "                \n",
    "                storage.add(prediction)\n",
    "                storage.add({'r': tensor(rewards).unsqueeze(-1).float().to(device),\n",
    "                             'm': tensor(1 - terminals).unsqueeze(-1).float().to(device),\n",
    "                             's': tensor(self.states).to(device)})\n",
    "                self.states = next_states\n",
    "                \n",
    "                for i, terminal in enumerate(terminals):\n",
    "                    if terminals[i]:\n",
    "                        self.episode_rewards.append(self.online_rewards[i])\n",
    "                        self.episode_rewards_window.append(self.online_rewards[i])\n",
    "                        self.online_rewards[i] = 0\n",
    "                        # reset the environment\n",
    "                        self.states = np.array(self.env.reset()).reshape(1,-1)\n",
    "\n",
    "            prediction = self.policy.act(self.states)\n",
    "            storage.add(prediction)\n",
    "            storage.placeholder()\n",
    "            \n",
    "            advantages = tensor(np.zeros((num_trajectories, 1))).float().to(device)\n",
    "            returns = prediction['v'].detach()\n",
    "            for i in reversed(range(rollout_length)):\n",
    "                returns = storage.r[i] + gamma * storage.m[i] * returns\n",
    "                if not use_gae:\n",
    "                    advantages = returns - storage.v[i].detach()\n",
    "                else:\n",
    "                    td_error = storage.r[i] + gamma * storage.m[i] * storage.v[i + 1] - storage.v[i]\n",
    "                    advantages = advantages * gae_tau * gamma * storage.m[i] + td_error\n",
    "                storage.adv[i] = advantages.detach()\n",
    "                storage.ret[i] = returns.detach()\n",
    "\n",
    "            states, actions, log_probs_old, returns, advantages = storage.cat(['s', 'a', 'log_pi_a', 'ret', 'adv'])\n",
    "            actions = actions.detach()\n",
    "            log_probs_old = log_probs_old.detach()\n",
    "            advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "            \n",
    "            ppo_ratio_clip = 0.2\n",
    "            gradient_clip = 0.5\n",
    "            entropy_weight = 0.0\n",
    "            \n",
    "            for _ in range(10):\n",
    "                sampler = random_sample(np.arange(states.size(0)), mini_batch_size)\n",
    "                for batch_indices in sampler:\n",
    "                    batch_indices = tensor(batch_indices).long()\n",
    "                    sampled_states = states[batch_indices]\n",
    "                    sampled_actions = actions[batch_indices]\n",
    "                    sampled_log_probs_old = log_probs_old[batch_indices]\n",
    "                    sampled_returns = returns[batch_indices]\n",
    "                    sampled_advantages = advantages[batch_indices]\n",
    "\n",
    "                    prediction = self.policy.act(sampled_states.cpu().numpy(), sampled_actions)\n",
    "                    ratio = (prediction['log_pi_a'] - sampled_log_probs_old).exp()\n",
    "                    obj = ratio * sampled_advantages\n",
    "                    obj_clipped = ratio.clamp(1.0 - ppo_ratio_clip,\n",
    "                                              1.0 + ppo_ratio_clip) * sampled_advantages\n",
    "                    policy_loss = -torch.min(obj, obj_clipped).mean() - entropy_weight * prediction['ent'].mean()\n",
    "\n",
    "                    value_loss = 0.5 * (sampled_returns - prediction['v']).pow(2).mean()\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    (policy_loss + value_loss).backward()\n",
    "                    nn.utils.clip_grad_norm_(self.policy.parameters(), gradient_clip)\n",
    "                    self.optimizer.step()\n",
    "            \n",
    "            #printing progress\n",
    "            if opt_step % PRINT_EVERY == 0:\n",
    "                print (\"Opt step: {}\\t Avg reward: {:.2f}\\t std: {}\".format(opt_step,np.mean(self.episode_rewards_window),\n",
    "                                                                             self.policy.std.item()))\n",
    "                # save the policy\n",
    "                torch.save(self.policy, 'ppo-crawler.policy')\n",
    "            \n",
    "            if np.mean(self.episode_rewards_window)>= target_score:\n",
    "                print (\"Environment solved in {} optimization steps! ... Avg reward : {:.2f}\".format(opt_step-100,\n",
    "                                                                                          np.mean(self.episode_rewards_window)))\n",
    "                # save the policy\n",
    "                torch.save(self.policy, 'ppo-crawler.policy')\n",
    "                break\n",
    "                \n",
    "        return self.episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets define and train our agent\n",
    "agent = Agent(env=env,learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opt step: 0\t Avg reward: -1128.27\t std: -0.0012511805398389697\n",
      "Opt step: 100\t Avg reward: -1008.77\t std: -0.1825476735830307\n",
      "Opt step: 200\t Avg reward: -795.97\t std: -0.4053317904472351\n",
      "Opt step: 300\t Avg reward: -720.59\t std: -0.5840398669242859\n",
      "Opt step: 400\t Avg reward: -618.01\t std: -0.7772052884101868\n",
      "Opt step: 500\t Avg reward: -709.36\t std: -0.8589652180671692\n",
      "Opt step: 600\t Avg reward: -490.45\t std: -0.9118962287902832\n",
      "Opt step: 700\t Avg reward: -507.58\t std: -0.9408363103866577\n",
      "Opt step: 800\t Avg reward: -304.37\t std: -0.9078362584114075\n"
     ]
    }
   ],
   "source": [
    "scores = agent.train(max_opt_steps=2000,gamma=0.99,target_score=-200,use_gae=False,PRINT_EVERY=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the reward curve\n",
    "fig = plt.figure()\n",
    "plt.plot(scores)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward Curve: PPO')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Watch the smart agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this cell to load the trained policy for Pendulum-v0\n",
    "# load policy\n",
    "policy =  torch.load('ppo-crawler.policy',map_location='cpu')\n",
    "agent = Agent(env)\n",
    "agent.policy = policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "total_rewards = np.zeros(12)\n",
    "\n",
    "# reset the environment\n",
    "states = np.array(env.reset())\n",
    "value = []\n",
    "r = []\n",
    "for t in range(2000):\n",
    "    prediction = agent.policy.act(states)\n",
    "    action  = prediction['a'].cpu().numpy()\n",
    "    v = prediction['v'].detach().cpu().numpy()\n",
    "    #frames.append(env.render(mode='rgb_array')) \n",
    "    \n",
    "    \n",
    "    # send all actions to tne environment\n",
    "    next_states,rewards,terminals,_ = env.step(list(action))\n",
    "\n",
    "    next_states = np.array(next_states)\n",
    "    rewards = np.array(rewards)                   \n",
    "    terminals = np.array(terminals)\n",
    "    \n",
    "    #value.append(v.squeeze())\n",
    "    #r.append(reward)\n",
    "    states=next_states\n",
    "    total_rewards+= rewards\n",
    "    if np.any(terminals):\n",
    "        for i,terminal in enumerate(terminals):\n",
    "            if terminal:\n",
    "                eps_reward = total_rewards[i]\n",
    "                break\n",
    "        break\n",
    "        \n",
    "print (\"Total reward:\",eps_reward)\n",
    "#animate_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(3).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reacher",
   "language": "python",
   "name": "reacher"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
